---
title: 本番レベルのLLMアプリケーションの設計原則
created_at: 2025-06-14
updated_at: 2025-06-15
---

# 本番レベルのLLMアプリケーションの設計原則

このドキュメントは、LLMアプリケーションをプロトタイプから本番で運用可能なレベルへと引き上げるための設計原則をまとめたものです。これは Heroku が提唱した [The Twelve-Factor App](https://12factor.net/) の思想を、LLMおよびエージェントの時代に合わせて再構成した**12の原則**です。

LLMエージェントを「本番品質」に引き上げる鍵は、**プロンプト・履歴・ツール呼び出しをコードと同格に扱い、ステートレスな純関数へ近づける設計と運用**にあります。これらの原則を守ることで、信頼性と拡張性を備えたLLMアプリケーションを構築できます。

## 背景と目的

LLMエージェントは「自然言語 ↔ ツール呼び出し」のループを通じて外部世界と対話しますが、実際の運用では長いワークフロー、障害からの復旧、複数ユーザーとの協調といった課題に直面します。Dex Horthy氏らが提唱した「12 Factor Agents」は、これらの課題に対応するためのガイドです。

オリジナルの12-Factor AppがSaaSの安定運用を目的としていたのに対し、エージェント版はLLM特有の制約（トークン上限、確率性）とツール実行の安全性に焦点を当てています。プロンプト、コンテキスト、制御フローを「コードと同格」に扱う点が最大の特徴です。

---

## 12の原則 概要

| 原則 | 目的・キーポイント |
| --- | --- |
| 1. **Natural Language → Tool Calls** | 自然言語を構造化された「ツール呼び出し」に変換し、決定論的なコードで実行する。 |
| 2. **Own Your Prompts** | プロンプトをブラックボックス化せず、テスト可能な「一級のコード」としてバージョン管理する。 |
| 3. **Own Your Context Window** | メッセージ形式を最適化し、全履歴を意識して入力コンテキストを圧縮する。 |
| 4. **Tools are Structured Outputs** | 「ツール」をLLMが返す構造化JSONそのものと捉え、呼び出し後の処理はアプリケーション側で定義する。 |
| 5. **Unify Execution State & Business State** | 実行状態と業務データを分離せず、スレッド（コンテキスト履歴）を「単一の真実」とする。 |
| 6. **Launch / Pause / Resume with Simple APIs** | エージェントをいつでも一時停止・再開可能にし、長時間ジョブや人間による確認を安全に組み込む。 |
| 7. **Contact Humans with Tool Calls** | 「人間に質問する」フローもツールとして定義し、構造化された形で支援を要請する。 |
| 8. **Own Your Control Flow** | ループ、リトライ、メモリ管理などの制御フローをフレームワーク任せにせず、明示的に設計する。 |
| 9. **Compact Errors into Context Window** | エラー情報を要約してコンテキストに含め、LLM自身の「自己修復能力」を活用する。 |
| 10. **Small, Focused Agents** | 1エージェントが担当するタスクは3〜20ターン程度に抑えます。これにより、LLMの集中力を維持し、デバッグを容易にします。 |
| 11. **Trigger from Anywhere** | Cron, Webhook, Chatなど、任意のイベントからエージェントを起動できるように設計し、ユーザーの利用シーンに合わせます。 |
| 12. **Make Your Agent a Stateless Reducer** | エージェントを「`(thread, new_event) -> new_thread`」という形式の純関数に近づけることで、テストの容易性と水平スケーラビリティが劇的に向上します。これは12-Factor Appの「プロセスはステートレスであれ」という原則と同じ思想です。ステートレスな（Stateless）状態遷移マシン（Reducer）を実現します。 |

---

## 12の原則 詳細

### 1. Natural Language → Tool Calls
- **要点**: 自然言語をまずJSONなどの構造化データへ変換します。LLMの役割を「どのツールを、どの引数で呼ぶか」の決定に限定することで、後続処理を決定論的なコードに委ねられます。
- **実装ヒント**:
  - 生成系API（OpenAI tool-calling, Anthropic function-callingなど）を直接利用すると実装がシンプルです。
  - ツールのスキーマをOpenAPIやPydanticで厳密に定義し、意図しないコマンド実行を静的型チェックと権限制御で防ぎます。

### 2. Own Your Prompts
- **要点**: プロンプトを「ブラックボックス」にせず、リポジトリでバージョン管理し、ユニットテストを義務付けます。
- **実装ヒント**:
  - `libs/prompts/`のような専用ディレクトリを設け、自動スナップショットテストを導入し、意図しない変更をCIで検出します。

### 3. Own Your Context Window
- **要点**: 標準的なチャットメッセージ形式に固執せず、YAMLやカスタムJSON形式で履歴を「圧縮」して1つのメッセージにまとめることで、トークン効率が向上します。
- **実装ヒント**:
  - 「システムプロンプト + 圧縮された履歴 + 最新のユーザー入力」という構成でLLMに情報を渡すのが典型的なパターンです。

### 4. Tools are Structured Outputs
- **要点**: 「ツール」とはLLMが返すJSONそのものです。実際のAPI呼び出しやDB操作は、そのJSONを解釈するアプリケーション側で自由にマッピングします。
- **実装ヒント**:
  - この原則は、原則8（制御フローを自前で持つ）と組み合わせることで真価を発揮します。

### 5. Unify Execution State & Business State
- **要点**: ワークフローの状態遷移と業務データを分離すると管理が複雑化します。「スレッド（履歴）」のみを「単一の真実」として扱うことで、実装が大幅に単純化されます。
- **実装ヒント**:
  - DBには `thread_id`, `messages[]`, `metadata` といったシンプルな構造のテーブルを用意し、再開時はこのスレッドを読むだけで済むようにします。

### 6. Launch / Pause / Resume with Simple APIs
- **要点**: 長時間処理や人間による確認が必要な場合に、エージェントを安全に一時停止し、外部イベント（API呼び出しなど）で再開できるインターフェースを用意します。
- **実装ヒント**:
  - `POST /threads/{id}/resume` のように、特定のスレッドを再開させるためのREST APIエンドポイントやキューを準備します。

### 7. Contact Humans with Tool Calls
- **要点**: 「人間に確認を求める」「質問する」といったフローも一つのツールとして定義し、Slack DMやEmailなどを通じて自動で通知します。
- **実装ヒント**:
  - 人間からの返答はそのままスレッドに追加し、原則6のAPIでエージェントを再開させることで、人間との協調ループを閉じます。

### 8. Own Your Control Flow
- **要点**: ループ回数、リトライ、指数関数的バックオフといった制御ロジックは、フレームワークのデフォルト設定に依存せず、アプリケーション側で明示的に実装します。
- **実装ヒント**:
  - 「1ターン = (LLM呼び出し + ツール実行 + 状態更新)」を一つの関数にまとめると、テストが容易になります。

### 9. Compact Errors into Context Window
- **要点**: 例外のスタックトレース全文は長すぎるため、要約したエラーメッセージや関連するコードスニペットだけをコンテキストに含めてLLMに再入力し、自己修復を促します。
- **実装ヒント**:
  - 失敗回数をスレッドのメタデータに記録し、設定した閾値を超えた場合は、原則7の仕組みを使って人間へエスカレーションします。

### 10. Small, Focused Agents
- **要点**: 1つのエージェントが担当するタスクは3〜20ターン程度に抑えます。これにより、LLMの集中力を維持し、デバッグを容易にします。
- **実装ヒント**:
  - 大きな目標は、まずPlanner（計画）エージェントがタスクを分解し、それぞれのサブタスクを専門のSub-Agent（実行）エージェントに割り当てるパターンで実装します。

### 11. Trigger from Anywhere
- **要点**: Cronジョブ、Webhook、Slackコマンドなど、様々なイベントを起点としてエージェントを起動できるように設計し、ユーザーの利用シーンに合わせます。
- **実装ヒント**:
  - 「イベント受信 → メッセージをスレッドに追加 → 再開APIを呼び出す」という流れに統一することで、実装をシンプルに保ちます。

### 12. Make Your Agent a Stateless Reducer
- **要点**: エージェントを「`(thread, new_event) -> new_thread`」という形式の純関数に近づけることで、テストの容易性と水平スケーラビリティが劇的に向上します。これは12-Factor Appの「プロセスはステートレスであれ」という原則と同じ思想です。
- **実装ヒント**:
  - ステートは永続化されたスレッドのみに依存させます。スレッド履歴はIDとJSONのペアでRDBやDynamoDBに保存し、悲観的/楽観的ロックで同時書き込みを制御します。
  - このアプローチにより、どのサーバーインスタンスがリクエストを処理しても同じ結果が得られ、障害からの復旧もスレッドを再実行するだけで可能になります。
- **コード例**:
  ```python
  # 純関数 (pure function): 同じ入力に対して常に同じ出力を返し、副作用を持たない。
  # テストが容易で、並列実行に強い。
  def agent(thread, event):
      prompt = build_prompt(thread, event)
      llm_out = call_llm(prompt)
      delta = extract_messages(llm_out)  # LLMが返す新しいメッセージ
      return thread + delta              # -> new_thread
  ```

---

## 12 Factor Agents と従来 12 Factor App の対比

| 分類 | 目的 | 対応する 12-Factor App 原則 |
| --- | --- | --- |
| **Factor 1 & 4** | 入出力の構造化 | **Dev/Prod parity** – 決定論的な差異を減らす |
| **Factor 6 & 11** | 柔軟な起動点 | **Port binding / Processes** – ステートレスプロセス |
| **Factor 12** | テスト容易性 & スケール | **Stateless processes** – 水平スケール前提 |

---

## まとめと実践へのヒント

**12 Factor Agents**は、LLMアプリケーションを本番環境で堅牢に運用するための設計指針です。プロンプト、コンテキスト、ツール呼び出し、さらには人間との連携までをコードとして管理し、テストと観測性を確保することを目的とします。

### 実践のステップ
1.  **部分導入から始める**:
    いきなり全てを導入するのではなく、まずは **原則1（ツール呼び出し）、原則4（構造化出力）、原則9（エラーの自己修復）** を既存のサービスに組み込むことから始めると、効果を体感しやすくなります。

2.  **観測性を確保する**:
    **原則5（スレッドを単一の真実とする）**の設計は、そのままデバッグ用のUIや監査ログに応用できます。開発の初期段階で、スレッド履歴をMarkdownやHTMLとして表示する機能を実装しておくと、後の開発と運用が格段に楽になります。

3.  **段階的に拡張する**:
    LLMの性能向上（コンテキスト長の増加など）を見越して、**原則10（小さなエージェント）** の考え方を採用し、最初は小さく、徐々にタスクの連鎖を増やしていく戦略が有効です。将来のために、エージェント間のメッセージング基盤をモジュール化しておくと良いでしょう。

### 結論
これらの原則をチェックリストとして活用し、段階的に適用することで、「プロトタイプ止まりのエージェント」から、**再現性が高く、壊れにくく、拡張しやすい本番レベルのLLMアプリケーション**へと着実に進化させることができます。